#cloud-config
# UpCloud Cloud-Init - LLM Provider Deployment
# Repository: https://github.com/Sahlulegat/llm-provider

package_update: true
package_upgrade: false # Prevents nvidia drivers from upgrading

packages:
  - curl
  - git
  - make
  - htop
  - net-tools
  - python3
  - python3-pip

write_files:
  - path: /etc/llm-provider-initialized
    content: |
      LLM Provider initialized from GitHub
      Repository: https://github.com/Sahlulegat/llm-provider
      Model: ${model_name}
      GPU Support: NVIDIA with CUDA
    owner: root:root
    permissions: '0644'

  - path: /etc/systemd/system/llm-provider.service
    content: |
      [Unit]
      Description=LLM Provider - Ollama and Open WebUI
      Requires=docker.service
      After=docker.service network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      WorkingDirectory=/opt/llm-provider
      ExecStart=/usr/bin/make start
      ExecStop=/usr/bin/docker-compose down
      User=root
      StandardOutput=journal
      StandardError=journal
      TimeoutStartSec=600
      LimitNOFILE=65536
      LimitNPROC=4096

      [Install]
      WantedBy=multi-user.target
    owner: root:root
    permissions: '0644'

  - path: /etc/systemd/system/llm-monitor-inactivity.service
    content: |
      [Unit]
      Description=Monitor LLM API inactivity for auto-shutdown
      After=network.target llm-provider.service
      Requires=llm-provider.service

      [Service]
      Type=oneshot
      Environment=INACTIVITY_TIMEOUT=${inactivity_timeout}
      ExecStart=/bin/bash /opt/llm-provider/scripts/monitor-inactivity.sh
      StandardOutput=journal
      StandardError=journal
      User=root

      [Install]
      WantedBy=multi-user.target
    owner: root:root
    permissions: '0644'

  - path: /etc/systemd/system/llm-monitor-inactivity.timer
    content: |
      [Unit]
      Description=Run LLM inactivity monitor every 5 minutes
      Requires=llm-monitor-inactivity.service

      [Timer]
      OnBootSec=10min
      OnUnitActiveSec=5min
      Unit=llm-monitor-inactivity.service

      [Install]
      WantedBy=timers.target
    owner: root:root
    permissions: '0644'

runcmd:
  # Configure llmadmin user (created by UpCloud login block)
  - usermod -aG sudo llmadmin || true
  - echo "llmadmin ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/llmadmin
  - chmod 440 /etc/sudoers.d/llmadmin

  # Configure floating IP if provided
  - |
    if [ -n "${floating_ip}" ]; then
      cat > /etc/netplan/99-floating-ip.yaml << 'EOFNETPLAN'
    network:
      version: 2
      renderer: networkd
      ethernets:
        eth0:
          addresses:
            - ${floating_ip}/32
    EOFNETPLAN
      chmod 600 /etc/netplan/99-floating-ip.yaml
      netplan apply
      echo "Floating IP ${floating_ip} configured" >> /var/log/cloud-init-output.log
    fi

  # Upgrade packages except nvidia drivers
  - apt-mark hold "nvidia*" "libnvidia*"
  - apt-get upgrade -y

  # Install Docker
  - curl -fsSL https://get.docker.com -o get-docker.sh
  - sh get-docker.sh
  - systemctl enable docker
  - systemctl start docker
  - rm get-docker.sh
  - usermod -aG docker llmadmin

  # Install NVIDIA Container Toolkit
  - curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
  - curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  - apt-get update
  - apt-get install -y nvidia-container-toolkit
  - nvidia-ctk runtime configure --runtime=docker
  - systemctl restart docker

  # Install Docker Compose V2
  - mkdir -p /usr/local/lib/docker/cli-plugins
  - curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o /usr/local/lib/docker/cli-plugins/docker-compose
  - chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
  - ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/bin/docker-compose

  # Clone project from GitHub
  - git clone https://github.com/Sahlulegat/llm-provider.git /opt/llm-provider

  # Generate Fernet key and create .env
  - pip3 install cryptography
  - |
    FERNET_KEY=$(python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
    cat > /opt/llm-provider/.env << EOFENV
    OLLAMA_PORT=${ollama_port}
    OLLAMA_ORIGINS=${ollama_origins}
    OLLAMA_KEEP_ALIVE=${ollama_keep_alive}
    OLLAMA_MAX_LOADED_MODELS=${ollama_max_loaded_models}
    OLLAMA_LOAD_TIMEOUT=${ollama_load_timeout}
    MODEL_NAME=${model_name}
    MODEL_PULL_ON_START=${model_pull_on_start}
    API_TIMEOUT=${api_timeout}
    LOG_LEVEL=${log_level}
    WEBUI_PORT=${webui_port}
    WEBUI_SECRET_KEY="$FERNET_KEY"
    WEBUI_NAME="${webui_name}"
    ENABLE_SIGNUP=${enable_signup}
    DEFAULT_USER_ROLE=${default_user_role}
    WEBUI_AUTH=${webui_auth}
    DOMAIN_NAME=${domain_name}
    ACME_EMAIL=${acme_email}
    EOFENV

  # Set permissions
  - chown -R llmadmin:llmadmin /opt/llm-provider
  - chmod 600 /opt/llm-provider/.env
  - chmod +x /opt/llm-provider/scripts/monitor-inactivity.sh

  # Enable and start services
  - systemctl daemon-reload
  - systemctl enable llm-provider.service
  - systemctl start llm-provider.service

  # Wait for Ollama to be ready and auto-pull model
  - echo "Waiting for Ollama to be ready..." >> /var/log/cloud-init-output.log
  - sleep 60
  - docker exec ollama-provider ollama pull ${model_name}
  - echo "Model ${model_name} pulled successfully at $(date)" >> /var/log/cloud-init-output.log

  # Enable and start inactivity monitor
  - systemctl enable llm-monitor-inactivity.timer
  - systemctl start llm-monitor-inactivity.timer
  - echo "Auto-shutdown monitoring configured (timeout: ${inactivity_timeout}s)" >> /var/log/cloud-init-output.log

  # Log completion
  - echo "LLM Provider setup completed at $(date)" >> /var/log/cloud-init-output.log

final_message: |
  ============================================
  LLM Provider Setup Complete!
  ============================================

  GitHub: https://github.com/Sahlulegat/llm-provider

  Services starting:
  - Ollama API: http://localhost:${ollama_port}
  - Open WebUI: http://localhost:${webui_port}

  Monitor: journalctl -u llm-provider.service -f
  GPU Status: nvidia-smi
  ============================================
