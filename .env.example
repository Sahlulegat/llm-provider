# Ollama Configuration
OLLAMA_PORT=11434
OLLAMA_ORIGINS=*
# Keep model loaded indefinitely (never unload), or use time like 30m
OLLAMA_KEEP_ALIVE=-1
OLLAMA_MAX_LOADED_MODELS=1
# Increase timeout for large model operations
OLLAMA_LOAD_TIMEOUT=10m

# Model Configuration
MODEL_NAME=gpt-oss:120b
MODEL_PULL_ON_START=true

# API Configuration
# Increased timeout for 120B model inference
API_TIMEOUT=600

# Logging
LOG_LEVEL=info

# Resource Configuration (for cloud deployment)
# Recommended: 64GB+ RAM, 200GB+ disk space

# ============================================
# UpCloud API Credentials (for Terraform)
# ============================================
UPCLOUD_USERNAME=your-api-username
UPCLOUD_PASSWORD=your-api-password

# ============================================
# Open WebUI Configuration
# ============================================
WEBUI_PORT=3000
# IMPORTANT: Change this to a secure random string in production!
# Generate with: python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
WEBUI_SECRET_KEY="#SECRET_KEY#"
WEBUI_NAME="LLM Chat"
# Allow user signup (set to false after creating admin account)
ENABLE_SIGNUP=true
# Default role for new users: pending, user, or admin
DEFAULT_USER_ROLE=pending
# Enable authentication (always true for security)
WEBUI_AUTH=true

# ============================================
# Traefik / HTTPS Configuration (for cloud deployment)
# ============================================
# Your domain name (e.g., llm.yourdomain.com)
DOMAIN_NAME=""
# Email for Let's Encrypt notifications
ACME_EMAIL=""
