#cloud-config
# UpCloud Cloud-Init - LLM Provider Deployment
# Repository: https://github.com/Sahlulegat/llm-provider

package_update: true
package_upgrade: true

packages:
  - curl
  - git
  - make
  - htop
  - net-tools
  - python3
  - python3-pip

users:
  - name: llmadmin
    groups: docker, sudo
    shell: /bin/bash
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    ssh_authorized_keys:
      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFNcHP8pgPPbrnD01OzGk82PNgY6guS4rwx0LGG1hggh pmollier@pack-solutions.com

runcmd:
  # Install Docker
  - curl -fsSL https://get.docker.com -o get-docker.sh
  - sh get-docker.sh
  - systemctl enable docker
  - systemctl start docker
  - rm get-docker.sh

  # Install NVIDIA Container Toolkit
  - curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
  - curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  - apt-get update
  - apt-get install -y nvidia-container-toolkit
  - nvidia-ctk runtime configure --runtime=docker
  - systemctl restart docker

  # Install Docker Compose V2
  - mkdir -p /usr/local/lib/docker/cli-plugins
  - curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o /usr/local/lib/docker/cli-plugins/docker-compose
  - chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
  - ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/bin/docker-compose

  # Clone project from GitHub
  - git clone https://github.com/Sahlulegat/llm-provider.git /opt/llm-provider
  - cd /opt/llm-provider

  # Generate Fernet key and create .env
  - pip3 install cryptography
  - |
    FERNET_KEY=$(python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
    cat > /opt/llm-provider/.env << EOF
    OLLAMA_PORT=11434
    OLLAMA_ORIGINS=*
    OLLAMA_KEEP_ALIVE=-1
    OLLAMA_MAX_LOADED_MODELS=1
    OLLAMA_LOAD_TIMEOUT=10m
    MODEL_NAME=gpt-oss:120b
    MODEL_PULL_ON_START=true
    API_TIMEOUT=600
    LOG_LEVEL=info
    WEBUI_PORT=3000
    WEBUI_SECRET_KEY="${FERNET_KEY}"
    WEBUI_NAME="LLM Chat"
    ENABLE_SIGNUP=true
    DEFAULT_USER_ROLE=pending
    WEBUI_AUTH=true
    DOMAIN_NAME=""
    ACME_EMAIL=""
    EOF

  # Set permissions
  - chown -R llmadmin:llmadmin /opt/llm-provider
  - chmod 600 /opt/llm-provider/.env

  # Create systemd service
  - |
    cat > /etc/systemd/system/llm-provider.service << 'EOF'
    [Unit]
    Description=LLM Provider - Ollama and Open WebUI
    Requires=docker.service
    After=docker.service network-online.target
    Wants=network-online.target

    [Service]
    Type=oneshot
    RemainAfterExit=yes
    WorkingDirectory=/opt/llm-provider
    ExecStart=/usr/bin/make start
    ExecStop=/usr/bin/docker-compose down
    User=root
    StandardOutput=journal
    StandardError=journal
    TimeoutStartSec=600
    LimitNOFILE=65536
    LimitNPROC=4096

    [Install]
    WantedBy=multi-user.target
    EOF

  # Enable and start service
  - systemctl daemon-reload
  - systemctl enable llm-provider.service
  - systemctl start llm-provider.service

  # Log completion
  - echo "LLM Provider setup completed at $(date)" >> /var/log/cloud-init-output.log

write_files:
  - path: /etc/llm-provider-initialized
    content: |
      LLM Provider initialized from GitHub
      Repository: https://github.com/Sahlulegat/llm-provider
      Model: gpt-oss:120b
      GPU Support: NVIDIA with CUDA
      Initialized at: $(date)
    owner: root:root
    permissions: '0644'

final_message: |
  ============================================
  LLM Provider Setup Complete!
  ============================================

  GitHub: https://github.com/Sahlulegat/llm-provider

  Services starting:
  - Ollama API: http://localhost:11434
  - Open WebUI: http://localhost:3000

  Monitor: journalctl -u llm-provider.service -f
  GPU Status: nvidia-smi
  ============================================
