# Ollama Configuration
OLLAMA_PORT=11434
OLLAMA_ORIGINS=*
# Keep model loaded indefinitely (never unload), or use time like 30m
OLLAMA_KEEP_ALIVE=-1
OLLAMA_MAX_LOADED_MODELS=1
# Increase timeout for large model operations
OLLAMA_LOAD_TIMEOUT=10m

# Model Configuration
MODEL_NAME=gpt-oss:120b
MODEL_PULL_ON_START=true

# API Configuration
# Increased timeout for 120B model inference
API_TIMEOUT=600

# Logging
LOG_LEVEL=info

# Resource Configuration (for cloud deployment)
# Recommended: 64GB+ RAM, 200GB+ disk space

# ============================================
# UpCloud API Credentials (for Terraform)
# ============================================
UPCLOUD_USERNAME=your-api-username
UPCLOUD_PASSWORD=your-api-password

# ============================================
# Open WebUI Configuration
# ============================================
WEBUI_PORT=3000

# WEBUI_SECRET_KEY : Clé de chiffrement Fernet pour Open WebUI
#
# EN LOCAL : Générez une clé manuellement avec:
#   python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#   Puis collez-la ci-dessous
#
# EN REMOTE (cloud-init) : Cette valeur sera automatiquement générée et remplacée
#   lors du déploiement. Laissez vide, le serveur la générera lui-même.
#
WEBUI_SECRET_KEY=""

WEBUI_NAME="LLM Chat"
# Allow user signup (set to false after creating admin account)
ENABLE_SIGNUP=true
# Default role for new users: pending, user, or admin
DEFAULT_USER_ROLE=pending
# Enable authentication (always true for security)
WEBUI_AUTH=true

# ============================================
# Production HTTPS Configuration (optional)
# ============================================
# Pour déploiement sur le web avec domaine personnalisé
# Laissez vide pour utilisation locale uniquement
#
# Votre nom de domaine (ex: chat.exemple.com)
DOMAIN_NAME=""
# Email pour les notifications Let's Encrypt
ACME_EMAIL=""
