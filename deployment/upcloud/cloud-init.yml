#cloud-config
# UpCloud Cloud-Init Configuration for LLM Provider
# This script automatically sets up the Ollama LLM provider on server startup

# Update system and install dependencies
package_update: true
package_upgrade: true

packages:
  - docker.io
  - docker-compose
  - curl
  - git
  - make
  - htop
  - net-tools

# Create user for running the application
users:
  - name: llmadmin
    groups: docker, sudo
    shell: /bin/bash
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    ssh_authorized_keys:
      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFNcHP8pgPPbrnD01OzGk82PNgY6guS4rwx0LGG1hggh pmollier@pack-solutions.com

# Configure Docker
runcmd:
  # Enable and start Docker
  - systemctl enable docker
  - systemctl start docker

  # Create application directory
  - mkdir -p /opt/llm-provider
  - cd /opt/llm-provider

  # Clone or create the LLM provider setup
  # Option 1: Clone from git repository (recommended)
  # - git clone https://github.com/yourusername/llm-provider.git /opt/llm-provider

  # Option 2: Create files directly (used here for standalone deployment)
  - |
    cat > /opt/llm-provider/.env << 'EOF'
    # Ollama Configuration
    OLLAMA_PORT=11434
    OLLAMA_ORIGINS=*
    # Keep model in memory longer for 120B model (expensive to reload)
    OLLAMA_KEEP_ALIVE=30m
    OLLAMA_MAX_LOADED_MODELS=1
    # Increase timeout for large model operations
    OLLAMA_LOAD_TIMEOUT=10m

    # Model Configuration
    MODEL_NAME=gpt-oss:120b
    MODEL_PULL_ON_START=true

    # API Configuration
    # Increased timeout for 120B model inference
    API_TIMEOUT=600

    # Logging
    LOG_LEVEL=info

    # Resource Configuration (for cloud deployment)
    # Recommended: 64GB+ RAM, 200GB+ disk space
    EOF

  - |
    cat > /opt/llm-provider/docker-compose.yml << 'EOF'
    version: '3.8'

    services:
      ollama:
        image: ollama/ollama:latest
        container_name: ollama-provider
        restart: unless-stopped

        # Port mapping
        ports:
          - "${OLLAMA_PORT:-11434}:11434"

        # Volume mounts
        volumes:
          - ./data/ollama:/root/.ollama
          - ./config:/config:ro
          - ./logs:/logs

        # Environment variables
        environment:
          - OLLAMA_HOST=0.0.0.0
          - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
          - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
          - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-1}

        # GPU support (uncomment for NVIDIA)
        # deploy:
        #   resources:
        #     reservations:
        #       devices:
        #         - driver: nvidia
        #           count: all
        #           capabilities: [gpu]

        # Healthcheck
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
          interval: 30s
          timeout: 10s
          retries: 3
          start_period: 40s

        networks:
          - llm-network

    networks:
      llm-network:
        driver: bridge
        name: llm-provider-network
    EOF

  - |
    cat > /opt/llm-provider/start.sh << 'EOF'
    #!/bin/bash
    set -e

    # Colors for output
    RED='\033[0;31m'
    GREEN='\033[0;32m'
    YELLOW='\033[1;33m'
    NC='\033[0m' # No Color

    echo -e "${GREEN}========================================${NC}"
    echo -e "${GREEN}   LLM Provider - Starting Ollama${NC}"
    echo -e "${GREEN}========================================${NC}"

    # Source environment variables
    source .env

    # Create necessary directories
    mkdir -p data/ollama logs config

    # Start Docker Compose
    echo -e "${GREEN}Starting Ollama container...${NC}"
    docker-compose up -d

    # Wait for Ollama to be ready
    echo -e "${YELLOW}Waiting for Ollama to be ready...${NC}"
    max_attempts=30
    attempt=0

    while [ $attempt -lt $max_attempts ]; do
        if curl -s http://localhost:${OLLAMA_PORT:-11434}/api/tags > /dev/null 2>&1; then
            echo -e "${GREEN}Ollama is ready!${NC}"
            break
        fi
        attempt=$((attempt + 1))
        echo -e "${YELLOW}Waiting... (attempt $attempt/$max_attempts)${NC}"
        sleep 2
    done

    if [ $attempt -eq $max_attempts ]; then
        echo -e "${RED}Failed to connect to Ollama after $max_attempts attempts${NC}"
        exit 1
    fi

    # Pull model if configured
    if [ "${MODEL_PULL_ON_START}" = "true" ]; then
        echo -e "${GREEN}Pulling model ${MODEL_NAME}...${NC}"
        echo -e "${YELLOW}This may take a while (120B model is ~100GB)...${NC}"
        docker-compose exec -T ollama ollama pull ${MODEL_NAME}
    fi

    echo -e "${GREEN}========================================${NC}"
    echo -e "${GREEN}   Ollama is running!${NC}"
    echo -e "${GREEN}   API endpoint: http://localhost:${OLLAMA_PORT:-11434}${NC}"
    echo -e "${GREEN}========================================${NC}"
    EOF

  - chmod +x /opt/llm-provider/start.sh

  # Create systemd service for auto-start
  - |
    cat > /etc/systemd/system/llm-provider.service << 'EOF'
    [Unit]
    Description=LLM Provider - Ollama Service
    Requires=docker.service
    After=docker.service network-online.target
    Wants=network-online.target

    [Service]
    Type=oneshot
    RemainAfterExit=yes
    WorkingDirectory=/opt/llm-provider
    ExecStart=/opt/llm-provider/start.sh
    ExecStop=/usr/bin/docker-compose down
    User=root
    StandardOutput=journal
    StandardError=journal

    # Resource limits
    LimitNOFILE=65536
    LimitNPROC=4096

    [Install]
    WantedBy=multi-user.target
    EOF

  # Enable and start the service
  - systemctl daemon-reload
  - systemctl enable llm-provider.service
  - systemctl start llm-provider.service

  # Set ownership
  - chown -R llmadmin:llmadmin /opt/llm-provider

  # Setup firewall (optional - uncomment if using UFW)
  # - ufw allow 22/tcp
  # - ufw allow 11434/tcp
  # - ufw --force enable

  # Log completion
  - echo "LLM Provider setup completed at $(date)" >> /var/log/cloud-init-output.log

# Write completion marker
write_files:
  - path: /etc/llm-provider-initialized
    content: |
      LLM Provider initialized at: $(date)
      Model: gpt-oss:120b
    owner: root:root
    permissions: '0644'

# Final message
final_message: |
  ============================================
  LLM Provider Setup Complete!
  ============================================

  The Ollama service is starting up and will:
  1. Start the Docker container
  2. Download the gpt-oss:120b model (~100GB)
  3. Be available on port 11434

  Monitor progress:
    - journalctl -u llm-provider.service -f
    - docker logs -f ollama-provider

  Test the API:
    curl http://localhost:11434/api/tags

  ============================================
