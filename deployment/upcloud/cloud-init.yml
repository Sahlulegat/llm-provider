#cloud-config
# UpCloud Cloud-Init - LLM Provider Deployment
# Repository: https://github.com/Sahlulegat/llm-provider

package_update: true
package_upgrade: false # Prevents nvidia drivers from upgrading

packages:
  - curl
  - git
  - make
  - htop
  - net-tools
  - python3
  - python3-pip

runcmd:
  # Configure llmadmin user (created by UpCloud login block)
  # Add to docker and sudo groups
  - usermod -aG sudo llmadmin || true
  - echo "llmadmin ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/llmadmin
  - chmod 440 /etc/sudoers.d/llmadmin

  # Upgrade packages except nvidia drivers (doesnt work otherwise)
  - apt-mark hold "nvidia*" "libnvidia*"
  - apt-get upgrade -y
  # Install Docker
  - curl -fsSL https://get.docker.com -o get-docker.sh
  - sh get-docker.sh
  - systemctl enable docker
  - systemctl start docker
  - rm get-docker.sh

  # Add llmadmin to docker group (now that docker is installed)
  - usermod -aG docker llmadmin

  # Install NVIDIA Container Toolkit
  - curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
  - curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
  - apt-get update
  - apt-get install -y nvidia-container-toolkit
  - nvidia-ctk runtime configure --runtime=docker
  - systemctl restart docker

  # Install Docker Compose V2
  - mkdir -p /usr/local/lib/docker/cli-plugins
  - curl -SL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-x86_64 -o /usr/local/lib/docker/cli-plugins/docker-compose
  - chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
  - ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/bin/docker-compose

  # Clone project from GitHub
  - git clone https://github.com/Sahlulegat/llm-provider.git /opt/llm-provider
  - cd /opt/llm-provider

  # Generate Fernet key and create .env
  - pip3 install cryptography
  - |
    FERNET_KEY=$(python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())")
    cat > /opt/llm-provider/.env << EOF
    OLLAMA_PORT=${ollama_port}
    OLLAMA_ORIGINS=${ollama_origins}
    OLLAMA_KEEP_ALIVE=${ollama_keep_alive}
    OLLAMA_MAX_LOADED_MODELS=${ollama_max_loaded_models}
    OLLAMA_LOAD_TIMEOUT=${ollama_load_timeout}
    MODEL_NAME=${model_name}
    MODEL_PULL_ON_START=${model_pull_on_start}
    API_TIMEOUT=${api_timeout}
    LOG_LEVEL=${log_level}
    WEBUI_PORT=${webui_port}
    WEBUI_SECRET_KEY="$FERNET_KEY"
    WEBUI_NAME="${webui_name}"
    ENABLE_SIGNUP=${enable_signup}
    DEFAULT_USER_ROLE=${default_user_role}
    WEBUI_AUTH=${webui_auth}
    DOMAIN_NAME=${domain_name}
    ACME_EMAIL=${acme_email}
    EOF

  # Set permissions
  - chown -R llmadmin:llmadmin /opt/llm-provider
  - chmod 600 /opt/llm-provider/.env

  # Create systemd service
  - |
    cat > /etc/systemd/system/llm-provider.service << 'EOF'
    [Unit]
    Description=LLM Provider - Ollama and Open WebUI
    Requires=docker.service
    After=docker.service network-online.target
    Wants=network-online.target

    [Service]
    Type=oneshot
    RemainAfterExit=yes
    WorkingDirectory=/opt/llm-provider
    ExecStart=/usr/bin/make start
    ExecStop=/usr/bin/docker-compose down
    User=root
    StandardOutput=journal
    StandardError=journal
    TimeoutStartSec=600
    LimitNOFILE=65536
    LimitNPROC=4096

    [Install]
    WantedBy=multi-user.target
    EOF

  # Enable and start service
  - systemctl daemon-reload
  - systemctl enable llm-provider.service
  - systemctl start llm-provider.service

  # Wait for Ollama to be ready and auto-pull model
  - echo "Waiting for Ollama to be ready..." >> /var/log/cloud-init-output.log
  - sleep 60
  - docker exec ollama-provider ollama pull ${model_name}
  - echo "Model ${model_name} pulled successfully at $(date)" >> /var/log/cloud-init-output.log

  # Log completion
  - echo "LLM Provider setup completed at $(date)" >> /var/log/cloud-init-output.log

write_files:
  - path: /etc/llm-provider-initialized
    content: |
      LLM Provider initialized from GitHub
      Repository: https://github.com/Sahlulegat/llm-provider
      Model: ${model_name}
      GPU Support: NVIDIA with CUDA
    owner: root:root
    permissions: '0644'

final_message: |
  ============================================
  LLM Provider Setup Complete!
  ============================================

  GitHub: https://github.com/Sahlulegat/llm-provider

  Services starting:
  - Ollama API: http://localhost:${ollama_port}
  - Open WebUI: http://localhost:${webui_port}

  Monitor: journalctl -u llm-provider.service -f
  GPU Status: nvidia-smi
  ============================================